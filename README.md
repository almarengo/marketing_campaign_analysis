# Marketing Campaign Analysis

![](images/marketing.jpg)



## Abstract: 

This project is a meant to review all the basic data science concepts including:

- EDA and Data Preprocesing
- Feature Engineering
- Data Normalization
- Assumptions of linearity and Linear Regression
- Class imbalance: oversampling and undersampling
- Logistic Regression
- Decision Trees
- Pre Pruning, Post Pruning
- Model evaluation and metrics
- Hyperparameters tuning
- Pipelines and Column Transformers
- Bagging and Boosting

Because of its great usability, we used a famous dataset from Kaggle: Marketing campaign. [Link here](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign)

<br/>


### Notebook 1: EDA and Linear Regression
This notebook contains the Exploratory Data Analysis (EDA), to dig deep into the data and extract usuful informations, and Feature Engineering. The second part of this notebook is dedicated to linearity: it's assumptions and prediction of cusomer's incomes using the linear regression model.


<br/>

### Notebook 2: Logistic Regression and model evaluation
In this notebook we'll look into classification. The first part is in common with the previous notebook (EDA and Feature Engineering) while in the second part we discuss class imbalance, oversampling and undersampling, logistic regression model, model evaluation, metric discussion and choosing the right threshold that suits the purpose of the model.


<br/>

### Notebook 3: Decision Trees (with pre/post-pruning techniques)
In this notebook, similar to what we did in notebook 2, we'll look into classification and in particular into Decision Trees. The first part is in common with the previous notebooks (EDA and Feature Engineering) while in the second part we discuss major differences between logistic regression and decision trees, hyperparameter tuning, cross validation, pre/post-pruning and metric discussion.


<br/>

### Notebook 4: Pipelines and Column Transformers
In this notebook, we run what we did in notebook 2, using column transformers and pipelines. We will illustrate how to apply different preprocessing and feature extraction pipelines to different subsets of features. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.


<br/>

### Notebook 5: Bagging and Boosting
In this notebook, similar to what we did in notebook 3, we'll run tree classifiers and in particula two ensemble methods: boosting wit XGBoost and badding with Random Forest. We will illustrate how these two methods perform against regular decision trees.


<br/>

## Conclusions
This is a good review of major data science concepts. For every data scientist out there that needs a good refresher around Machine Learning. 